---
---
@article{li2025diffusion,
  title   = {Diffusion language models know the answer before decoding},
  author  = {Li, Pengxiang and Zhou, Yefan and Muhtar, Dilxat and Yin, Lu and Yan, Shilin and Shen, Li and Vosoughi, Soroush and Liu, Shiwei},
  year    = {2026},
  venue   = {ICLR 2026 Oral},
  paper   = {https://arxiv.org/pdf/2508.19982},
  code    = {https://github.com/pixeli99/Prophet},
  models  = {https://huggingface.co/...}
  }

@article{pelleriti2025neural,
  title  = {Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers},
  author = {Pelleriti, Nico and Spiegel, Christoph and Liu, Shiwei and Mart{\'\i}nez-Rubio, David and Zimmer, Max and Pokutta, Sebastian},
  year   = {2026},
  venue  = {ICLR 2026},
  paper  = {https://arxiv.org/pdf/2510.13444},
  code   = {https://github.com/ZIB-IOL/Neural-Sum-of-Squares},
  models = {https://huggingface.co/…},
}

@article{su2025gptailor,
  title={GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching},
  author={Su, Guinan and Shen, Li and Yin, Lu and Liu, Shiwei and Yang, Yanwu and Geiping, Jonas},
  year   = {2026},
  venue  = {ICLR 2026},
  paper  = {https://arxiv.org/pdf/2506.20480},
  code   = {https://github.com/Guinan-Su/auto-merge-llm},
  models = {https://huggingface.co/…}
}

@article{sun2025curse,
  title    = {The Curse of Depth in Large Language Models},
  author   = {Sun, Wenfang and Song, Xinyuan and Li, Pengxiang and Yin, Lu and Zheng, Yefeng and Liu, Shiwei},
  year     = {2025},
  venue    = {NeurIPS 2025},
  paper    = {https://arxiv.org/pdf/2502.05795},
  code     = {https://github.com/lmsdss/LayerNorm-Scaling},
  models   = {https://huggingface.co/pengxiang/LNS_1B}
}

@article{he2025alphadecay,
  title   = {AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs},
  author  = {He, Di and Jaiswal, Ajay and Tu, Songjun and Shen, Li and Yuan, Ganzhao and Liu, Shiwei and Yin, Lu},
  year     = {2025},
  venue    = {NeurIPS 2025},
  paper    = {https://arxiv.org/pdf/2506.14562},
  code     = {https://github.com/hed-ucas/AlphaDecay},
  models = {https://huggingface.co/…}
}

@article{chen2025gpas,
  title={GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling},
  author={Chen, Tianhao and Xu, Xin and Liu, Zijing and Li, Pengxiang and Song, Xinyuan and Jaiswal, Ajay Kumar and Zhang, Fan and Hu, Jishan and Wang, Yang and Chen, Hao and others},
  year     = {2025},
  venue    = {NeurIPS 2025},
  paper    = {https://arxiv.org/pdf/2506.22049?},
  code     = {https://github.com/dandingsky/GPAS},
  models = {https://huggingface.co/…}
}

@article{liu2025lift,
  title    = {LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning},
  author   = {Liu, Zihang and Pang, Tianyu and Balabanov, Oleg and Yang, Chaoqun and Huang, Tianjin and Yin, Lu and Yang, Yaoqing and Liu, Shiwei},
  year     = {2025},
  venue    = {ICML 2025},
  paper    = {https://arxiv.org/pdf/2506.00772},
  code     = {https://github.com/zihanghliu/LIFT},
  models = {https://huggingface.co/…}
}

@article{zhuang2025mask,
  title    = {Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More},
  author   = {Zhuang, Xialie and Jia, Zhikai and Li, Jianjin and Zhang, Zhenyu and Shen, Li and Cao, Zheng and Liu, Shiwei},
  year     = {2025},
  venue    = {ICML 2025},
  paper    = {https://arxiv.org/abs/2502.07490},
  code     = {https://github.com/scitix/MEAP},
  models = {https://huggingface.co/…}
}

@article{li2025sos1,
  title    = {SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers},
  author   = {Li, Kechen and Zhu, Wenqi and Cartis, Coralia and Ji, Tianbo and Liu, Shiwei},
  year     = {2025},
  venue     = {Preprint},
  paper    = {https://arxiv.org/abs/2502.20545}
}

@article{huang2025stable,
  title    = {Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam},
  author   = {Huang, Tianjin and Hu, Haotian and Zhang, Zhenyu and Jin, Gaojie and Li, Xiang and Shen, Li and Chen, Tianlong and Liu, Lu and Wen, Qingsong and Wang, Zhangyang and Liu, Shiwei},
  year     = {2025},
  venue     = {Preprint},
  paper    = {https://arxiv.org/abs/2502.17055}
}

@article{li2024mix,
  title    = {Mix-ln: Unleashing the power of deeper layers by combining pre-ln and post-ln},
  author   = {Li, Pengxiang and Yin, Lu and Liu, Shiwei},
  year     = {2025},
  venue    = {ICLR 2025},
  paper    = {https://arxiv.org/abs/2412.13795},
  code     = {https://github.com/pixeli99/MixLN},
  models = {https://huggingface.co/…}
}

@article{huang2025spam,
  title    = {SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training},
  author   = {Huang, Tianjin and Zhu, Ziquan and Jin, Gaojie and Liu, Lu and Wang, Zhangyang and Liu, Shiwei},
  year     = {2025},
  venue    = {ICLR 2025},
  paper    = {https://arxiv.org/abs/2501.06842},
  code     = {https://github.com/TianjinYellow/SPAM-Optimizer},
  models = {https://huggingface.co/…}
}

@article{li2025outlier,
  title={Outlier-weighed layerwise sampling for LLM fine-tuning},
  author={Li, Pengxiang and Yin, Lu and Gao, Xiaowei and Liu, Shiwei},
  venue={Findings of the Association for Computational Linguistics: ACL 2025},
  paper={https://arxiv.org/abs/2405.18380},
  code     = {https://github.com/pixeli99/OWS},
  models = {https://huggingface.co/…},
  year={2025}
}

@article{li2024owlore,
  title    = {OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning},
  author   = {Li, Pengxiang and Yin, Lu and Gao, Xiaowei and Liu, Shiwei},
  year     = {2024},
  venue    = {ICLR 2025},
  paper    = {https://arxiv.org/abs/2501.06842},
  code     = {https://github.com/TianjinYellow/SPAM-Optimizer},
  models = {https://huggingface.co/…},
  paper    = {https://arxiv.org/abs/2405.18380}
}

@article{yin2023outlier,
  title    = {Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author   = {Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  year     = {2024},
  venue    = {ICML 2024},
  paper    = {https://arxiv.org/abs/2310.05175},
  code     = {https://github.com/luuyin/OWL},
  models = {https://huggingface.co/…}
}

@inproceedings{zhangcam,
  title    = {CaM: Cache Merging for Memory-efficient LLMs Inference},
  author   = {Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  year     = {2024},
  booktitle= {Forty-first International Conference on Machine Learning},
  abbr     = {ICML2024},
  venue    = {ICML 2024}
}

@inproceedings{yinjunk,
  title    = {Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $$\backslash$textit $\{$Irreversibly$\}$ $ and $$\backslash$textit $\{$Monotonically$\}$ $ Impairs``Difficult" Downstream Tasks in LLMs},
  author   = {Yin, Lu and JAISWAL, AJAY KUMAR and Liu, Shiwei and Kundu, Souvik and Wang, Zhangyang},
  year     = {2024},
  booktitle= {Forty-first International Conference on Machine Learning},
  abbr     = {ICML2024},
  venue    = {ICML 2024}
}

@article{xiao2024dynamic,
  title   = {Dynamic Data Pruning for Automatic Speech Recognition},
  author  = {Xiao, Qiao and Ma, Pingchuan and Fernandez-Lopez, Adriana and Wu, Boqian and Yin, Lu and Petridis, Stavros and Pechenizkiy, Mykola and Pantic, Maja and Mocanu, Decebal Constantin and Liu, Shiwei},
  year    = {2024},
  journal = {arXiv preprint arXiv:2406.18373},
  abbr    = {Interspeech2024},
  venue   = {Interspeech 2024},
  paper   = {https://arxiv.org/abs/2406.18373}
}

@article{fernandez2024msrs,
  title   = {MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization},
  author  = {Fernandez-Lopez, Adriana and Chen, Honglie and Ma, Pingchuan and Yin, Lu and Xiao, Qiao and Petridis, Stavros and Liu, Shiwei and Pantic, Maja},
  year    = {2024},
  journal = {arXiv preprint arXiv:2406.17614},
  abbr    = {Interspeech2024},
  venue   = {Interspeech 2024},
  paper   = {https://arxiv.org/abs/2406.17614}
}

@article{zhang2023dynamic,
  title   = {Dynamic sparse no training: Training-free fine-tuning for sparse llms},
  author  = {Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong},
  year    = {2024},
  journal = {arXiv preprint arXiv:2310.08915},
  abbr    = {ICLR2024},
  venue   = {ICLR 2024},
  paper   = {https://arxiv.org/abs/2310.08915},
  code     = {https://github.com/zyxxmu/DSnoT}
}

@article{yang2023adamerging,
  title   = {Adamerging: Adaptive model merging for multi-task learning},
  author  = {Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  year    = {2024},
  abbr    = {ICLR2024},
  venue   = {ICLR 2024},
  paper   = {https://arxiv.org/abs/2310.02575},
  code     = {https://github.com/EnnengYang/AdaMerging}
}

@article{liu2023don,
  title     = {Don’t be so dense: Sparse-to-sparse gan training without sacrificing performance},
  author    = {Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  year      = {2023},
  journal   = {International Journal of Computer Vision},
  volume    = {131},
  number    = {10},
  pages     = {2635--2648},
  publisher = {Springer},
  abbr      = {IJCV},
  venue     = {IJCV}
}

@article{liu2022more,
  title    = {More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author   = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\"a}rkk{\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  year     = {2023},
  abbr     = {ICLR2023},
  selected = {true},
  venue    = {ICLR 2023},
  paper    = {https://arxiv.org/abs/2207.03620},
  code     = {https://github.com/VITA-Group/SLaK/issues}
}

@article{hoang2023revisiting,
  title  = {Revisiting pruning at initialization through the lens of ramanujan graph},
  author = {Hoang, Duc NM and Liu, Shiwei and Marculescu, Radu and Wang, Zhangyang},
  year   = {2023},
  abbr   = {ICLR2023},
  venue  = {ICLR 2023},
  paper    = {https://openreview.net/forum?id=uVcDssQff_},
  code    = {https://github.com/VITA-Group/ramanujan-on-pai}
}

@article{chen2023sparse,
  title  = {Sparse moe as the new dropout: Scaling dense and self-slimmable transformers},
  author = {Chen, Tianlong and Zhang, Zhenyu and Jaiswal, Ajay and Liu, Shiwei and Wang, Zhangyang},
  year   = {2023},
  venue  = {ICLR 2023},
  paper    = {https://arxiv.org/abs/2303.01610},
  code    = {https://github.com/VITA-Group/Random-MoE-as-Dropout}
}

@article{liu2023sparsity,
  title  = {Sparsity may cry: Let us fail (current) sparse neural networks together!},
  author = {Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
  year   = {2023},
  abbr   = {ICLR2023},
  venue  = {ICLR 2023},
  paper    = {https://arxiv.org/abs/2303.02141},
  code    = {https://github.com/VITA-Group/SMC-Bench},
  models  = {https://huggingface.co/...}
}

@article{huang2022you,
  title    = {You can have better graph neural networks by not training weights at all: Finding untrained gnns tickets},
  author   = {Huang, Tianjin and Chen, Tianlong and Fang, Meng and Menkovski, Vlado and Zhao, Jiaxu and Yin, Lu and Pei, Yulong and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola and others},
  year     = {2022},
  abbr     = {LoG2022},
  selected = {true},
  code     = {https://github.com/TianjinYellow/UGTs-LoG},
  venue    = {LoG 2022}
}

@article{liu2022unreasonable,
  title    = {The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author   = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  year     = {2022},
  venue    = {ICLR 2022},
  paper    = {https://arxiv.org/abs/2202.02643},
  code    = {https://github.com/VITA-Group/FreeTickets},
  models  = {https://huggingface.co/...}
}

@article{liu2021deep,
  title   = {Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity},
  author  = {Liu, Shiwei and Chen, Tianlong and Atashgahi, Zahra and Chen, Xiaohan and Sokar, Ghada and Mocanu, Elena and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  year    = {2022},
  journal = {arXiv preprint arXiv:2106.14568},
  abbr    = {ICLR2022},
  venue   = {ICLR 2022},
  paper   = {https://arxiv.org/abs/2106.14568}
}

@article{liu2021sparse,
  title    = {Sparse training via boosting pruning plasticity with neuroregeneration},
  author   = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  year     = {2021},
  venue   = {NeurIPS 2021},
  paper   = {https://arxiv.org/abs/2106.10404},
  code    = {https://github.com/VITA-Group/GraNet},
  models  = {https://huggingface.co/...}
}

@inproceedings{liu2021we,
  title     = {Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author    = {Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  year      = {2021},
  venue   = {ICML 2021},
  paper   = {https://arxiv.org/abs/2102.02887},
  code    = {https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization},
  models  = {https://huggingface.co/...}
}

@inproceedings{liu2021selfish,
  title     = {Selfish sparse rnn training},
  author    = {Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  year      = {2021},
  venue   = {ICML 2021},
  paper   = {https://arxiv.org/abs/2101.09048},
  code    = {https://github.com/Shiweiliuiiiiiii/Selfish-RNN},
  models  = {https://huggingface.co/...}
}

