---
---
@article{li2025diffusion,
  title   = {Diffusion language models know the answer before decoding},
  author  = {Li, Pengxiang and Zhou, Yefan and Muhtar, Dilxat and Yin, Lu and Yan, Shilin and Shen, Li and Vosoughi, Soroush and Liu, Shiwei},
  year    = {2026},
  venue   = {ICLR 2026},
  paper   = {https://arxiv.org/pdf/2508.19982},
  code    = {https://github.com/pixeli99/Prophet},
  models  = {https://huggingface.co/...}
  }


@article{sun2025curse,
  title={The Curse of Depth in Large Language Models},
  author={Sun, Wenfang and Song, Xinyuan and Li, Pengxiang and Yin, Lu and Zheng, Yefeng and Liu, Shiwei},
  journal={arXiv preprint arXiv:2502.05795},
      abbr = {NeurIPS2025},
      selected={true},
  year={2025}
}

@article{he2025alphadecay,
  title={AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs},
  author={He, Di and Jaiswal, Ajay and Tu, Songjun and Shen, Li and Yuan, Ganzhao and Liu, Shiwei and Yin, Lu},
  journal={arXiv preprint arXiv:2506.14562},
      abbr = {NeurIPS2025},
  year={2025}
}

@article{liu2025lift,
  title={LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning},
  author={Liu, Zihang and Pang, Tianyu and Balabanov, Oleg and Yang, Chaoqun and Huang, Tianjin and Yin, Lu and Yang, Yaoqing and Liu, Shiwei},
  journal={arXiv preprint arXiv:2506.14562},
      abbr = {ICML2025},
      selected={true},
  year={2025}
}

@article{zhuang2025mask,
  title={Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More},
  author={Zhuang, Xialie and Jia, Zhikai and Li, Jianjin and Zhang, Zhenyu and Shen, Li and Cao, Zheng and Liu, Shiwei},
  journal={arXiv preprint arXiv:2502.07490},
      abbr = {ICML2025},
      selected={true},
  year={2025}
}

@article{li2025sos1,
  title={SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers},
  author={Li, Kechen and Zhu, Wenqi and Cartis, Coralia and Ji, Tianbo and Liu, Shiwei},
  journal={arXiv preprint arXiv:2502.20545},
            abbr = {Preprint},
      selected={true},
  year={2025}
}

@article{huang2025stable,
  title={Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam},
  author={Huang, Tianjin and Hu, Haotian and Zhang, Zhenyu and Jin, Gaojie and Li, Xiang and Shen, Li and Chen, Tianlong and Liu, Lu and Wen, Qingsong and Wang, Zhangyang and  Liu, Shiwei},
  journal={arXiv preprint arXiv:2502.17055},
        abbr = {Preprint},
      selected={true},
  year={2025}
}


@article{li2024mix,
  title={Mix-ln: Unleashing the power of deeper layers by combining pre-ln and post-ln},
  author={Li, Pengxiang and Yin, Lu and Liu, Shiwei},
  journal={arXiv preprint arXiv:2412.13795},
      abbr = {ICLR2025},
      selected={true},
  year={2024}
}

@article{huang2025spam,
  title={SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training},
  author={Huang, Tianjin and Zhu, Ziquan and Jin, Gaojie and Liu, Lu and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2501.06842},
        abbr = {ICLR2025},
      selected={true},
  year={2025}
}

@article{kolbeinsson2024composable,
  title={Composable interventions for language models},
  author={Kolbeinsson, Arinbjorn and O'Brien, Kyle and Huang, Tianjin and Gao, Shanghua and Liu, Shiwei and Schwarz, Jonathan Richard and Vaidya, Anurag and Mahmood, Faisal and Zitnik, Marinka and Chen, Tianlong and others},
  journal={arXiv preprint arXiv:2407.06483},
          abbr = {ICLR2025},
  year={2024}
}

@article{yang2025revisiting,
  title={Revisiting Flatness-aware Optimization in Continual Learning with Orthogonal Gradient Projection},
  author={Yang, Enneng and Shen, Li and Wang, Zhenyi and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  abbr = {TPAMI},
  year={2025}
}

@article{li2024owlore,
  title={OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning},
  author={Li, Pengxiang and Yin, Lu and Gao, Xiaowei and Liu, Shiwei},
  journal={arXiv preprint arXiv:2405.18380},
    abbr = {Preprint},
    selected={true},
  year={2024}
}

@article{zhang2024q,
  title={Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients},
  author={Zhang, Zhenyu and Jaiswal, Ajay and Yin, Lu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.08296},
      abbr = {CPAL2025},
  year={2024}
}

@article{jaiswal2024galore,
  title={From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients},
  author={Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.11239},
        abbr = {Preprint},
  year={2024}
}

@article{zhang2024q,
  title={Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache},
  author={Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={381--394},
        abbr = {MLSys2024},
  year={2024}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  booktitle={Forty-first International Conference on Machine Learning},
          abbr = {ICML2024},
              selected={true},
  year={2024}
}

@inproceedings{zhangcam,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Forty-first International Conference on Machine Learning},
            abbr = {ICML2024},
              year={2024}
}

@inproceedings{yinjunk,
  title={Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $$\backslash$textit $\{$Irreversibly$\}$ $ and $$\backslash$textit $\{$Monotonically$\}$ $ Impairs``Difficult" Downstream Tasks in LLMs},
  author={Yin, Lu and JAISWAL, AJAY KUMAR and Liu, Shiwei and Kundu, Souvik and Wang, Zhangyang},
  booktitle={Forty-first International Conference on Machine Learning},
              abbr = {ICML2024},
              year={2024}
}

@article{xiao2024dynamic,
  title={Dynamic Data Pruning for Automatic Speech Recognition},
  author={Xiao, Qiao and Ma, Pingchuan and Fernandez-Lopez, Adriana and Wu, Boqian and Yin, Lu and Petridis, Stavros and Pechenizkiy, Mykola and Pantic, Maja and Mocanu, Decebal Constantin and Liu, Shiwei},
  journal={arXiv preprint arXiv:2406.18373},
   abbr = {Interspeech2024},
  year={2024}
}

@article{fernandez2024msrs,
  title={MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization},
  author={Fernandez-Lopez, Adriana and Chen, Honglie and Ma, Pingchuan and Yin, Lu and Xiao, Qiao and Petridis, Stavros and Liu, Shiwei and Pantic, Maja},
  journal={arXiv preprint arXiv:2406.17614},
     abbr = {Interspeech2024},
  year={2024}
}

@article{zhang2023dynamic,
  title={Dynamic sparse no training: Training-free fine-tuning for sparse llms},
  author={Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2310.08915},
       abbr = {ICLR2024},
  year={2024}
}

@article{yang2023adamerging,
  title={Adamerging: Adaptive model merging for multi-task learning},
  author={Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.02575},
        abbr = {ICLR2024},
  year={2024}
}

@article{liu2023don,
  title={Donâ€™t be so dense: Sparse-to-sparse gan training without sacrificing performance},
  author={Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  journal={International Journal of Computer Vision},
  volume={131},
  number={10},
  pages={2635--2648},
  year={2023},
  publisher={Springer},
        abbr = {IJCV},
}

@article{liu2022more,
  title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\"a}rkk{\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2207.03620},
  abbr = {ICLR2023},
  selected={true},
  year={2023}
}

@article{hoang2023revisiting,
  title={Revisiting pruning at initialization through the lens of ramanujan graph},
  author={Hoang, Duc NM and Liu, Shiwei and  Marculescu, Radu and Wang Zhangyang},
  abbr = {ICLR2023},
  year={2023}
}

@article{chen2023sparse,
  title={Sparse moe as the new dropout: Scaling dense and self-slimmable transformers},
  author={Chen, Tianlong and Zhang, Zhenyu and Jaiswal, Ajay and Liu, Shiwei and Wang, Zhangyang},
      abbr = {ICLR2023},
        year={2023}
}

@article{liu2023sparsity,
  title={Sparsity may cry: Let us fail (current) sparse neural networks together!},
  author={Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
      abbr = {ICLR2023},
             year={2023}
}

@article{huang2022you,
  title={You can have better graph neural networks by not training weights at all: Finding untrained gnns tickets},
  author={Huang, Tianjin and Chen, Tianlong and Fang, Meng and Menkovski, Vlado and Zhao, Jiaxu and Yin, Lu and Pei, Yulong and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola and others},
        abbr = {LoG2022},
        selected={true},
  year={2022}
}

@article{liu2022unreasonable,
  title={The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2202.02643},
     abbr = {ICLR2022},
     selected={true},
  year={2022}
}

@article{liu2021deep,
  title={Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Atashgahi, Zahra and Chen, Xiaohan and Sokar, Ghada and Mocanu, Elena and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={arXiv preprint arXiv:2106.14568},
   abbr = {ICLR2022},
  year={2022}
}

@article{liu2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9908--9922},
  selected={true},
    abbr = {NeurIPS2021},
  year={2021}
}

@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6989--7000},
  year={2021},
    abbr = {ICML2021},
    selected={true},
  organization={PMLR}
}

@inproceedings{liu2021selfish,
  title={Selfish sparse rnn training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6893--6904},
  year={2021},
     abbr = {ICML2021},
  organization={PMLR}
}


